{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ed452ba",
   "metadata": {},
   "source": [
    "## Get Insight from your Business Data - Build LLM application with Fine Tuning using Hugging Face - Ashish Kumar Jain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ab8e05",
   "metadata": {},
   "source": [
    "### For improving the performance of the model on a particular task, we need to do training of the base model (per-trained Model like GPT4) with labeled data on single task. Base LLM Model is already per-trained on vast majority of unstructured textual data via self supervised learning where as fine tuning is a supervised learning process where we use a data set of labeled examples to update the weights of the exiting base LLM for a particular task.The training data set contains the prompt completion pairs with different examples, fine tuning process train the model with this data set to improve its ability to generate good completion for a specific task.The fine tuning which updates all the weights of the model is called full fine tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846c6108",
   "metadata": {},
   "source": [
    "### In the notebook we will use Hugging Face, it is a platform where machine learning community collaborates on models, datasets and applications. We will use Hugging Face to download one of the open source LLM model FLAN_T5 from Google . We will load this model from the local machine. You can easily download these model from Hugging Face by cloning the model repository. You can also download the General Knowledge dataset for the training the model.It will help you out to run this code without internet or in very constrained environment. Downloading model can take time depending on your network speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176bc693-b33c-433f-8974-39c3fb22c047",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install --upgrade pip\n",
    "! pip install 'transformers[torch]'\n",
    "! pip install datasets\n",
    "! pip install evaluate==0.4.0\n",
    "! pip install rouge_score==0.1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529ac025",
   "metadata": {},
   "source": [
    "#### Hugging Face provides Datasets library for easily accessing and sharing datasets. We can load the dataset in a single line of code from multiple sources (Hugging Face hub, local files systems and memory etc) in different formats (CSV, JSON, parquet, arrow, sql for reading from database etc) and use its powerful data processing methods to quickly get our dataset ready for training for LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d85a736-fd8a-408e-8313-d7f4341b6efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data_files = {\"train\":\"dataset/GK/train.json\",\n",
    "              \"test\":\"dataset/GK/validation.json\",\n",
    "              \"validation\":\"dataset/GK/test.json\"\n",
    "             }\n",
    "dataset = load_dataset(\"json\",data_files = data_files,field=\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b7cbb3",
   "metadata": {},
   "source": [
    "#### Hugging Face provides tokenizer class which is in charge of preparing the inputs for a model. We will use open source FLAN-T5-LARGE model from Hugging Face and load from local. It is a good encode-decoder instruct model. It shows good capability in many tasks\n",
    "\n",
    "#### You can easily download flan-t5-large model from Hugging Face by cloning the model repository. \n",
    "#### git clone https://huggingface.co/google/flan-t5-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e76c1e9-5e22-486c-8a2c-f29318f9862b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "modelPath = \"model/flan-t5-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(modelPath)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(modelPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c676ed",
   "metadata": {},
   "source": [
    "#### We will create instructed dataset for the training. We will to convert the question-answer pairs into explicit instructions for the LLM. Lets create a prompt instruction having instruction start and end of prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366a311b-1ce3-4005-8563-82af6a38f5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_generator(batchData):\n",
    "    start = 'Assuming you are working as General Knowladge instructor. Can you please answer the below question?\\n\\n'\n",
    "    end = '\\n Answer: '\n",
    "    training_prompt = [start + question + end for question in batchData['Question']]\n",
    "    batchData['input_ids'] = tokenizer(training_prompt, padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "    batchData['labels'] = tokenizer(batchData['Answer'], padding=\"max_length\", return_tensors=\"pt\").input_ids\n",
    "    return batchData\n",
    "\n",
    "instructed_datasets = dataset.map(prompt_generator, batched=True)\n",
    "instructed_datasets = instructed_datasets.remove_columns(['id','Question', 'Answer'])\n",
    "#print(instructed_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0bd2fa",
   "metadata": {},
   "source": [
    "#### We will use the PyTorch framework for fine tuning the model. The Hugging Face Trainer class provides an API for feature-complete training in PyTorch for most standard use cases. Before instantiating our Trainer object, we will create a TrainingArguments to access all the points of customization during training. In below code i am defining only 1 epoch for model training, you can choose no of epochs and other training parameter based on your compute, memory available and based on the final model evaluation result.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6525094b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import time\n",
    "\n",
    "output_dir = f'./model/trained-model-output/flan-output-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=1e-5, \n",
    "    num_train_epochs=1, \n",
    "    weight_decay=0.01,\n",
    "    max_steps =1\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=base_model,\n",
    "    args=training_args,\n",
    "    train_dataset=instructed_datasets['train'],\n",
    "    eval_dataset=instructed_datasets['validation']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2413c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e052309e",
   "metadata": {},
   "source": [
    "#### After training you can save the instructed model for future evaluation and inference use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2ce997",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_dir = f'./model/trained-model/flan-trained-{str(int(time.time()))}'\n",
    "tokenizer.save_pretrained(saved_dir)\n",
    "base_model.save_pretrained(saved_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94763bda",
   "metadata": {},
   "source": [
    "#### We can load the saved fine tuned instructed model from local file system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be85d25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruct_model = AutoModelForSeq2SeqLM.from_pretrained(\"model/trained-model/flan-trained-1692510911\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aafd3ce",
   "metadata": {},
   "source": [
    "#### For Generative AI applications, a qualitative approach where we ask our-self the question \"Is my model behaving right way?\" is usually a good starting point. We can see that by manually seeing the difference between actual answer with answers given by the instructed model. We can use our test dataset for evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb81876",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig\n",
    "import pandas as pd\n",
    "\n",
    "questions = dataset['test']['Question']\n",
    "actual_answers = dataset['test']['Answer']\n",
    "instruct_model_answers = []\n",
    "\n",
    "for _, question in enumerate(questions):\n",
    "    prompt = f\"\"\"\n",
    "\n",
    "Assuming you are working as General Knowladge instructor. Can you please answer the below question?\n",
    "\n",
    "{question}\n",
    "Answer:\"\"\";\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "    instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "    instruct_model_answers.append(instruct_model_text_output)\n",
    "    \n",
    "answers = list(zip(questions,actual_answers,instruct_model_answers))\n",
    "df = pd.DataFrame(answers, columns = ['question','actual answer','instruct model answer'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2ea4f",
   "metadata": {},
   "source": [
    "#### Other evaluating approach is qualitative. The ROUGE metric helps quantify the validity of answers produced by models. It compares answers to a actual answer which is part of our test dataset.You can read more about this from ROUGE metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8669b2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load('rouge')\n",
    "instruct_model_results = rouge.compute(\n",
    "    predictions=instruct_model_answers,\n",
    "    references=actual_answers,\n",
    "    use_aggregator=True,\n",
    "    use_stemmer=True,\n",
    ")\n",
    "print(instruct_model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf827ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
